{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4f28f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from transformers import ResNetModel\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b723e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10800\n",
      "1351\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "base_path = 'archive/'\n",
    "img_path = f'{base_path}Food Images/Food Images/'\n",
    "cap_path = f'{base_path}Food Ingredients and Recipe Dataset with Image Name Mapping.csv'\n",
    "\n",
    "data = pd.read_csv(cap_path)\n",
    "partitions = np.load(\"datasets/Food_Images/food_partitions.npy\", allow_pickle=True).item()\n",
    "print(len(partitions[\"train\"]))  # Access train partition\n",
    "print(len(partitions[\"test\"]))   # Access test partition\n",
    "print(len(partitions[\"valid\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff827a9",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4a6613d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10797\n"
     ]
    }
   ],
   "source": [
    "dropped_indices = data[data[\"Title\"].isna()].index  # Get indices of dropped rows\n",
    "partitions['train'] = [idx for idx in partitions['train'] if idx not in dropped_indices]\n",
    "print(len(partitions['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7aef343d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13496"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna(subset=[\"Title\"])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "84d9d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13582\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_folder = f\"archive/Food Images/Food Images\"\n",
    "\n",
    "valid_images = list({os.path.splitext(f)[0] for f in os.listdir(image_folder)})\n",
    "\n",
    "print(len(valid_images))\n",
    "      \n",
    "data = data[data[\"Image_Name\"].isin(valid_images)]\n",
    "\n",
    "# Reset index after filtering\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "427ed956",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = set(data.index)  # These are the indices that remain after filtering\n",
    "\n",
    "partitions['train'] = [idx for idx in partitions['train'] if idx in valid_indices]\n",
    "partitions['valid'] = [idx for idx in partitions['valid'] if idx in valid_indices]\n",
    "partitions['test'] = [idx for idx in partitions['test'] if idx in valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d393b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Normalize and remove unwanted characters\n",
    "def clean_text(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)  # Normalize Unicode\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")  # Remove non-ASCII chars\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the Title column\n",
    "data[\"Title\"] = data[\"Title\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Extract unique characters\n",
    "chars = list(set(\"\".join(data[\"Title\"])))\n",
    "\n",
    "# Ensure special tokens are first\n",
    "chars = ['<SOS>', '<EOS>', '<PAD>'] + sorted(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1b09b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = ['<SOS>', '<EOS>', '<PAD>', ' ', '!', '\"', '#', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    " \n",
    "NUM_CHAR = len(chars)\n",
    "idx2char = {k: v for k, v in enumerate(chars)}\n",
    "char2idx = {v: k for k, v in enumerate(chars)}\n",
    "\n",
    "TEXT_MAX_LEN = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2ff6590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, partition):\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        self.num_captions = 5\n",
    "        self.max_len = TEXT_MAX_LEN\n",
    "        self.img_proc = torch.nn.Sequential(\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224), antialias=True),\n",
    "            v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.partition[idx]  # Row index in dataset\n",
    "        item = self.data.iloc[real_idx]  # Get row\n",
    "                \n",
    "        img_name = item.Image_Name + '.jpg'\n",
    "        # print(img_name)\n",
    "        img = Image.open(f'{img_path}{img_name}').convert('RGB')\n",
    "        img = self.img_proc(img)\n",
    "        \n",
    "        caption = item[\"Title\"]\n",
    "        cap_list = list(caption)\n",
    "\n",
    "        final_list = [chars[0]]\n",
    "        final_list.extend(cap_list)\n",
    "        final_list.extend([chars[1]])\n",
    "        gap = self.max_len - len(final_list)\n",
    "        final_list.extend([chars[2]]*gap)\n",
    "\n",
    "        missing_chars = [c for c in final_list if c not in char2idx]\n",
    "        if missing_chars:\n",
    "            print(f\"Missing characters: {set(missing_chars)}\")\n",
    "\n",
    "        for char in missing_chars:\n",
    "            if char not in char2idx:\n",
    "                char2idx[char] = len(char2idx)  # Assign a new index\n",
    "\n",
    "        cap_idx = [char2idx[i] for i in final_list]\n",
    "\n",
    "        # return img, cap_idx\n",
    "        return img, torch.tensor(cap_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b652abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "        self.num_classes = NUM_CHAR\n",
    "\n",
    "    def forward(self, img, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img)\n",
    "        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        start_token = torch.full((batch_size,), char2idx['<SOS>'], dtype=torch.long, device=DEVICE)\n",
    "        start_embed = self.embed(start_token).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        hidden = feat\n",
    "        inp = start_embed\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(TEXT_MAX_LEN):  # Exclude <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            logits = self.proj(out[-1])  # (batch, NUM_CHAR)\n",
    "\n",
    "            outputs.append(logits.unsqueeze(1))  # Store timestep output\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            if captions is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                inp = self.embed(captions[:, t]).unsqueeze(0)  # Use ground truth token\n",
    "            else:\n",
    "                pred = logits.argmax(dim=1)\n",
    "                inp = self.embed(pred).unsqueeze(0)  # Use model prediction\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, NUM_CHAR)\n",
    "        return outputs.permute(0, 2, 1)  # (batch, NUM_CHAR, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "82acf476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_19764\\108332190.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  caption1 = torch.tensor(caption1)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_19764\\108332190.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  caption2 = torch.tensor(caption2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1853, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''A simple example to calculate loss of a single batch (size 2)'''\n",
    "dataset = Data(data, partitions['train'])\n",
    "\n",
    "img1, caption1 = next(iter(dataset))\n",
    "\n",
    "img2, caption2 = next(iter(dataset))\n",
    "\n",
    "caption1 = torch.tensor(caption1)\n",
    "caption2 = torch.tensor(caption2)\n",
    "img = torch.cat((img1.unsqueeze(0), img2.unsqueeze(0)))\n",
    "caption = torch.cat((caption1.unsqueeze(0), caption2.unsqueeze(0)))\n",
    "img, caption = img.to(DEVICE), caption.to(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "pred = model(img)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "loss = crit(pred, caption)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d3606f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.5946035575013605,\n",
       "  'precisions': [0.875, 0.7142857142857143, 0.5, 0.4],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 1.0,\n",
       "  'translation_length': 8,\n",
       "  'reference_length': 8},\n",
       " {'rouge1': 0.8571428571428571,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.8571428571428571,\n",
       "  'rougeLsum': 0.8571428571428571},\n",
       " {'meteor': 0.864795918367347})"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''metrics'''\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "reference = [['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .']]\n",
    "prediction = ['A girl goes into a wooden building .']\n",
    "\n",
    "res_b = bleu.compute(predictions=prediction, references=reference)\n",
    "res_r = rouge.compute(predictions=prediction, references=reference)\n",
    "res_m = meteor.compute(predictions=prediction, references=reference)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7595f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.4723665527410147,\n",
       "  'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       "  'brevity_penalty': 0.4723665527410147,\n",
       "  'length_ratio': 0.5714285714285714,\n",
       "  'translation_length': 4,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.7272727272727273,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.7272727272727273,\n",
       "  'rougeLsum': 0.7272727272727273},\n",
       " {'meteor': 0.5923507462686567})"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is running']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fe24da77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 1.0, 1.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.5, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.44612794612794615})"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ca1afd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 0.5, 0.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.25, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.3872053872053872},\n",
       " {'meteor': 0.45454545454545453})"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "res_m_sin = meteor.compute(predictions=pred1, references=ref, gamma=0) # no penalty by setting gamma to 0\n",
    "\n",
    "res_b, res_r, res_m, res_m_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bac05",
   "metadata": {},
   "source": [
    "Final metric we use for challenge 3: BLEU1, BLEU2, ROUGE-L, METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5433823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLEU-1:26.4%, BLEU2:18.6%, ROUGE-L:60.0%, METEOR:38.7%'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "bleu1 = bleu.compute(predictions=pred1, references=ref, max_order=1)\n",
    "bleu2 = bleu.compute(predictions=pred1, references=ref, max_order=2)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "f\"BLEU-1:{bleu1['bleu']*100:.1f}%, BLEU2:{bleu2['bleu']*100:.1f}%, ROUGE-L:{res_r['rougeL']*100:.1f}%, METEOR:{res_m['meteor']*100:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf5d9d",
   "metadata": {},
   "source": [
    "Now it is your turn! Try to finish the code below to run the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ad20cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "        self.num_classes = NUM_CHAR\n",
    "\n",
    "    def forward(self, img, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img)\n",
    "        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        start_token = torch.full((batch_size,), char2idx['<SOS>'], dtype=torch.long, device=DEVICE)\n",
    "        start_embed = self.embed(start_token).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        hidden = feat\n",
    "        inp = start_embed\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(TEXT_MAX_LEN):  # Exclude <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            logits = self.proj(out[-1])  # (batch, NUM_CHAR)\n",
    "\n",
    "            outputs.append(logits.unsqueeze(1))  # Store timestep output\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            if captions is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                inp = self.embed(captions[:, t]).unsqueeze(0)  # Use ground truth token\n",
    "            else:\n",
    "                pred = logits.argmax(dim=1)\n",
    "                inp = self.embed(pred).unsqueeze(0)  # Use model prediction\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, NUM_CHAR)\n",
    "        return outputs.permute(0, 2, 1)  # (batch, NUM_CHAR, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb649f03",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3f85e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def decode_caption(indices, vocab):\n",
    "    return ''.join([vocab[idx] if idx < len(vocab) else '<UNK>' for idx in indices if idx not in [0]])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes padding and special tokens, then strips whitespace.\"\"\"\n",
    "    return text.replace(\"<PAD>\", \"\").replace(\"<EOS>\", \"\").strip()\n",
    "\n",
    "def is_empty_prediction(pred_list):\n",
    "    \"\"\"Checks if any cleaned prediction is empty.\"\"\"\n",
    "    return any(len(clean_text(pred)) == 0 for pred in pred_list)\n",
    "\n",
    "def train(EPOCHS, batch_size=16, patience=5, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    wandb.init(project=\"captioning-model\", config={\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"teacher_forcing_ratio\": teacher_forcing_ratio\n",
    "    })\n",
    "\n",
    "    data_train = Data(data, partitions['train'])\n",
    "    data_valid = Data(data, partitions['valid'])\n",
    "    \n",
    "    dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dataloader_valid = DataLoader(data_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"DataLoader process is finished\")\n",
    "    \n",
    "    # model = Model().to(DEVICE)\n",
    "    model = Model(mode=\"word\").to(DEVICE)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate decay\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    os.makedirs(\"models/text_representation\", exist_ok=True)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Starting epoch {epoch+1}\")\n",
    "        model.train()\n",
    "        train_loss, train_acc = train_one_epoch(model, optimizer, crit, dataloader_train, teacher_forcing_ratio)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_valid)\n",
    "        print(f'Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            # \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Validation Loss\": valid_loss,\n",
    "            \"BLEU-1\": bleu1_score,\n",
    "            \"BLEU-2\": bleu2_score,\n",
    "            \"ROUGE-L\": rouge_score,\n",
    "            \"METEOR\": meteor_score\n",
    "        })\n",
    "\n",
    "        torch.save(model.state_dict(), f\"models/text_representation/best_model_{epoch + 1}.pth\")\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, crit, dataloader, teacher_forcing_ratio):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    bleu1_score = 0\n",
    "    bleu2_score = 0\n",
    "    rouge_score = 0\n",
    "    meteor_score = 0\n",
    "\n",
    "    for imgs, captions in dataloader:\n",
    "        imgs, captions = imgs.to(DEVICE), captions.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "        outputs = model(imgs, captions if use_teacher_forcing else None)\n",
    "\n",
    "        loss = crit(outputs, captions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        decoded_refs = [clean_text(decode_caption(caption.cpu().numpy(), chars)) for caption in captions]\n",
    "        decoded_preds = [clean_text(decode_caption(pred.cpu().numpy(), chars)) for pred in predicted]\n",
    "        \n",
    "        print(f\"Ref: {decoded_refs}\")\n",
    "        print(f\"Pred: {decoded_preds}\")\n",
    "\n",
    "        if is_empty_prediction(decoded_preds):\n",
    "            continue\n",
    "\n",
    "        bleu1 = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_refs], max_order=1)\n",
    "        bleu2 = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_refs], max_order=2)\n",
    "        res_r = rouge.compute(predictions=decoded_preds, references=decoded_refs)\n",
    "        res_m = meteor.compute(predictions=decoded_preds, references=decoded_refs)\n",
    "\n",
    "        # Accumulate scores\n",
    "        bleu1_score += bleu1[\"bleu\"]\n",
    "        bleu2_score += bleu2[\"bleu\"]\n",
    "        rouge_score += res_r[\"rougeL\"]\n",
    "        meteor_score += res_m[\"meteor\"]\n",
    "\n",
    "        print(f\"BLEU-1: {bleu1['bleu']:.4f}, BLEU-2: {bleu2['bleu']:.4f}, ROUGE-L: {res_r['rougeL']:.4f}, METEOR: {res_m['meteor']:.4f}\")\n",
    "        \n",
    "        print(\"_\" * 100)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    bleu1_score /= len(dataloader)\n",
    "    bleu2_score /= len(dataloader)\n",
    "    rouge_score /= len(dataloader)\n",
    "    meteor_score /= len(dataloader)\n",
    "\n",
    "    print(f\"BLEU-1: {bleu1_score:.4f}, BLEU-2: {bleu2_score:.4f}, ROUGE-L: {rouge_score:.4f}, METEOR: {meteor_score:.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader), 100\n",
    "\n",
    "\n",
    "def eval_epoch(model, crit, dataloader):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    bleu1_score = 0\n",
    "    bleu2_score = 0\n",
    "    rouge_score = 0\n",
    "    meteor_score = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, captions in dataloader:\n",
    "            imgs, captions = imgs.to(DEVICE), captions.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            loss = crit(outputs, captions)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            decoded_refs = [clean_text(decode_caption(caption.cpu().numpy(), chars)) for caption in captions]\n",
    "            decoded_preds = [clean_text(decode_caption(pred.cpu().numpy(), chars)) for pred in predicted]\n",
    "            \n",
    "            if is_empty_prediction(decoded_preds):\n",
    "                continue\n",
    "            \n",
    "            bleu1 = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_refs], max_order=1)\n",
    "            bleu2 = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_refs], max_order=2)\n",
    "            res_r = rouge.compute(predictions=decoded_preds, references=decoded_refs)\n",
    "            res_m = meteor.compute(predictions=decoded_preds, references=decoded_refs)\n",
    "\n",
    "            # Accumulate scores\n",
    "            bleu1_score += bleu1[\"bleu\"]\n",
    "            bleu2_score += bleu2[\"bleu\"]\n",
    "            rouge_score += res_r[\"rougeL\"]\n",
    "            meteor_score += res_m[\"meteor\"]\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    bleu1_score /= len(dataloader)\n",
    "    bleu2_score /= len(dataloader)\n",
    "    rouge_score /= len(dataloader)\n",
    "    meteor_score /= len(dataloader)\n",
    "\n",
    "    return avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 8\n",
    "data_test = Data(data, partitions['test'])   \n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "model = Model().to(DEVICE)\n",
    "model.load_state_dict(torch.load(f\"models/best_model.pth\", map_location=DEVICE))\n",
    "\n",
    "avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "print(f\"METEOR: {meteor_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.0001\n",
      "BLEU-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "METEOR: 0.0012\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(DEVICE)  # Do NOT load weights\n",
    "\n",
    "avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "\n",
    "print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "print(f\"METEOR: {meteor_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebc31b",
   "metadata": {},
   "source": [
    "# Experiments (ResNet-18 & LSTM Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f4263fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Better pooling\n",
    "\n",
    "        self.lstm = nn.LSTM(512, 512, num_layers=3, dropout=0.3, bidirectional=True)  # 3-layer Bi-LSTM\n",
    "        self.proj = nn.Linear(1024, NUM_CHAR)  # Adjust for bidirectional output\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "        self.num_classes = NUM_CHAR\n",
    "\n",
    "        # Trainable LSTM hidden state\n",
    "        self.hidden_init = nn.Parameter(torch.zeros(3 * 2, 1, 512))  # (num_layers * 2, batch, hidden_size)\n",
    "        self.cell_init = nn.Parameter(torch.zeros(3 * 2, 1, 512))\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(1024)  # Normalize LSTM outputs\n",
    "\n",
    "    def forward(self, img, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img).last_hidden_state  # Use full feature maps\n",
    "        feat = self.adaptive_pool(feat).squeeze(-1).squeeze(-1).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        start_token = torch.full((batch_size,), char2idx['<SOS>'], dtype=torch.long, device=DEVICE)\n",
    "        start_embed = self.embed(start_token).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        hidden = self.hidden_init.expand(-1, batch_size, -1).contiguous()  # Expand for batch size\n",
    "        cell = self.cell_init.expand(-1, batch_size, -1).contiguous()\n",
    "\n",
    "        inp = start_embed\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(TEXT_MAX_LEN):\n",
    "            out, (hidden, cell) = self.lstm(inp, (hidden, cell))\n",
    "            out = self.layer_norm(out)  # Apply layer normalization\n",
    "            logits = self.proj(out[-1])  # (batch, NUM_CHAR)\n",
    "\n",
    "            outputs.append(logits.unsqueeze(1))\n",
    "\n",
    "            # Scheduled Sampling: Reduce teacher forcing ratio gradually\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            if captions is not None and teacher_force:\n",
    "                inp = self.embed(captions[:, t]).unsqueeze(0)\n",
    "            else:\n",
    "                pred = logits.argmax(dim=1)\n",
    "                inp = self.embed(pred).unsqueeze(0)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, NUM_CHAR)\n",
    "        return outputs.permute(0, 2, 1)  # (batch, NUM_CHAR, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "wandb.init(project=\"C5_W3\")\n",
    "\n",
    "train(20)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18069341",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 8\n",
    "data_test = Data(data, partitions['test'])   \n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "model_list = [1, 3, 4, 6, 7, 11, 12, 14, 15, 16, 19]\n",
    "for idx in model_list:\n",
    "    print(f\"models/lstm_20/best_model_{idx}.pth\")\n",
    "\n",
    "    model = Model().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"models/lstm_20/best_model_{idx}.pth\", map_location=DEVICE))\n",
    "\n",
    "    avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "    print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "    print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "    print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "    print(f\"METEOR: {meteor_score:.4f}\")\n",
    "    print(50 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0da7e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.0004\n",
      "BLEU-2: 0.0000\n",
      "ROUGE-L: 0.0001\n",
      "METEOR: 0.0011\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(DEVICE)  # Do NOT load weights\n",
    "\n",
    "avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "\n",
    "print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "print(f\"METEOR: {meteor_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac694c5",
   "metadata": {},
   "source": [
    "# VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615a8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg19 = models.vgg19(pretrained=True).features\n",
    "        self.vgg19 = nn.Sequential(vgg19, nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        \n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "\n",
    "    def forward(self, img, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = img.shape[0]\n",
    "        \n",
    "        feat = self.vgg19(img)\n",
    "        feat = feat.view(1, batch_size, 512)\n",
    "        \n",
    "        start_token = torch.tensor(char2idx['<SOS>']).to(DEVICE)\n",
    "        start_embed = self.embed(start_token).repeat(batch_size, 1).unsqueeze(0)\n",
    "        \n",
    "        hidden = feat\n",
    "        inp = start_embed\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(TEXT_MAX_LEN):  # Excluding <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            proj_out = self.proj(out[-1])  # (batch, NUM_CHAR)\n",
    "            outputs.append(proj_out.unsqueeze(1))\n",
    "            \n",
    "            pred_token = proj_out.argmax(1)\n",
    "            pred_embed = self.embed(pred_token).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "            # Teacher forcing\n",
    "            if captions is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                next_token = captions[:, t]  # Use ground truth\n",
    "            else:\n",
    "                next_token = pred_token  # Use model prediction\n",
    "            \n",
    "            inp = self.embed(next_token).unsqueeze(0)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d6db5",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a458390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Vocabulary Size: 7627\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize at word level\n",
    "word_counter = Counter()\n",
    "for title in data[\"Title\"]:\n",
    "    words = title.split()  # Simple whitespace tokenization\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Create a word2idx dictionary\n",
    "word2idx = {word: idx for idx, (word, _) in enumerate(word_counter.items(), start=4)}\n",
    "\n",
    "# Add special tokens\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<SOS>'] = 1\n",
    "word2idx['<EOS>'] = 2\n",
    "word2idx['<UNK>'] = 3\n",
    "\n",
    "# Get NUM_WORDS\n",
    "NUM_WORDS = len(word2idx)\n",
    "print(f\"Word-Level Vocabulary Size: {NUM_WORDS}\")\n",
    "\n",
    "# Convert list to sorted format for consistency\n",
    "words = ['<SOS>', '<EOS>', '<PAD>', '<UNK>'] + sorted(word2idx.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728448b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6794"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_words(words, special_tokens=None):\n",
    "    if special_tokens is None:\n",
    "        special_tokens = {'<PAD>', '<SOS>', '<EOS>', '<UNK>'}  # Special tokens to keep\n",
    "\n",
    "    cleaned_words = set(special_tokens)  # Ensure special tokens are preserved\n",
    "\n",
    "    for word in words:\n",
    "        if word in special_tokens:  # Keep special tokens unchanged\n",
    "            continue\n",
    "\n",
    "        word = unicodedata.normalize(\"NFKD\", word)  # Normalize Unicode\n",
    "        word = re.sub(r'[^a-zA-Z\\s-]', '', word)  # Remove special characters & numbers\n",
    "        word = word.strip('-')  # Remove leading/trailing hyphens\n",
    "        word = word.strip()  # Trim spaces\n",
    "\n",
    "        if word:  # Only keep non-empty words\n",
    "            cleaned_words.add(word)\n",
    "\n",
    "    return sorted(cleaned_words)  # Return sorted unique words\n",
    "\n",
    "words = clean_words(words)\n",
    "NUM_WORDS = len(words)\n",
    "NUM_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772d2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece Vocabulary Size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "NUM_WORDPIECE = tokenizer.vocab_size\n",
    "\n",
    "print(f\"WordPiece Vocabulary Size: {NUM_WORDPIECE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ef1ec3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, partition):\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        self.num_captions = 5\n",
    "        self.max_len = TEXT_MAX_LEN\n",
    "        self.img_proc = torch.nn.Sequential(\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224), antialias=True),\n",
    "            v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.partition[idx]  # Row index in dataset\n",
    "        item = self.data.iloc[real_idx]  # Get row\n",
    "                \n",
    "        img_name = item.Image_Name + '.jpg'\n",
    "        # print(img_name)\n",
    "        img = Image.open(f'{img_path}{img_name}').convert('RGB')\n",
    "        img = self.img_proc(img)\n",
    "        \n",
    "        caption = item[\"Title\"]\n",
    "        cap_list = list(caption)\n",
    "\n",
    "        final_list = [chars[0]]\n",
    "        final_list.extend(cap_list)\n",
    "        final_list.extend([chars[1]])\n",
    "        gap = self.max_len - len(final_list)\n",
    "        final_list.extend([chars[2]]*gap)\n",
    "\n",
    "        missing_chars = [c for c in final_list if c not in char2idx]\n",
    "        if missing_chars:\n",
    "            print(f\"Missing characters: {set(missing_chars)}\")\n",
    "\n",
    "        for char in missing_chars:\n",
    "            if char not in char2idx:\n",
    "                char2idx[char] = len(char2idx)  # Assign a new index\n",
    "\n",
    "        cap_idx = [char2idx[i] for i in final_list]\n",
    "\n",
    "        # return img, cap_idx\n",
    "        return img, torch.tensor(cap_idx, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b501a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-Level Model: 81 tokens\n",
      "Word-Level Model: 6794 tokens\n",
      "WordPiece-Level Model: 30522 tokens\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, mode=\"char\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine vocabulary size\n",
    "        if mode == \"char\":\n",
    "            self.vocab_size = NUM_CHAR\n",
    "            self.token2idx = char2idx\n",
    "        elif mode == \"word\":\n",
    "            self.vocab_size = NUM_WORDS\n",
    "            self.token2idx = word2idx\n",
    "        elif mode == \"wordpiece\":\n",
    "            self.vocab_size = NUM_WORDPIECE\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose from 'char', 'word', 'wordpiece'.\")\n",
    "\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        \n",
    "        for param in self.resnet.parameters():  # Freeze all ResNet layers\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Better pooling\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, 512)\n",
    "        self.lstm = nn.LSTM(512, 512, num_layers=3, dropout=0.3, bidirectional=True)\n",
    "        self.proj = nn.Linear(1024, self.vocab_size)\n",
    "\n",
    "        self.hidden_init = nn.Parameter(torch.zeros(3 * 2, 1, 512))  # (num_layers * 2, batch, hidden_size)\n",
    "        self.cell_init = nn.Parameter(torch.zeros(3 * 2, 1, 512))\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(1024)  # Normalize LSTM outputs\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, img, captions=None, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img).last_hidden_state  # Use full feature maps\n",
    "\n",
    "        feat = self.adaptive_pool(feat).squeeze(-1).squeeze(-1).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        start_token = torch.full((batch_size,), self.token2idx['<SOS>'], dtype=torch.long, device=DEVICE)\n",
    "        start_embed = self.embed(start_token).unsqueeze(0)  # (1, batch, 512)\n",
    "\n",
    "        hidden = self.hidden_init.expand(-1, batch_size, -1).contiguous()\n",
    "        cell = self.cell_init.expand(-1, batch_size, -1).contiguous()\n",
    "\n",
    "        inp = start_embed\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(TEXT_MAX_LEN):  # TEXT_MAX_LEN (Set a max sequence length)\n",
    "            out, (hidden, cell) = self.lstm(inp, (hidden, cell))\n",
    "            out = self.layer_norm(out)  # Apply layer normalization\n",
    "            logits = self.proj(out[-1])  # (batch, vocab_size)\n",
    "\n",
    "            outputs.append(logits.unsqueeze(1))\n",
    "\n",
    "            # Scheduled Sampling\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            if captions is not None and teacher_force:\n",
    "                inp = self.embed(captions[:, t]).unsqueeze(0)\n",
    "            else:\n",
    "                pred = logits.argmax(dim=1)\n",
    "                inp = self.embed(pred).unsqueeze(0)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, vocab_size)\n",
    "        return outputs.permute(0, 2, 1)  # (batch, vocab_size, seq_len)\n",
    "\n",
    "print(f\"Char-Level Model: {NUM_CHAR} tokens\")\n",
    "print(f\"Word-Level Model: {NUM_WORDS} tokens\")\n",
    "print(f\"WordPiece-Level Model: {NUM_WORDPIECE} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00cfbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n",
    "wandb.init(project=\"C5_W3\")\n",
    "\n",
    "train(3)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103be404",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data_test = Data(data, partitions['test'])   \n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "model_list = [1, 2, 3]\n",
    "for idx in model_list:\n",
    "    print(f\"models/text_representation/best_model_{idx}.pth\")\n",
    "\n",
    "    # model = Model(\"word\").to(DEVICE)\n",
    "    model = Model(mode=\"word\").to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"models/text_representation/best_model_{idx}.pth\", map_location=DEVICE))\n",
    "\n",
    "    avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "    print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "    print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "    print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "    print(f\"METEOR: {meteor_score:.4f}\")\n",
    "    print(50 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0edca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model().to(DEVICE)  # Do NOT load weights\n",
    "model = Model(mode=\"word\").to(DEVICE)\n",
    "\n",
    "avg_loss, bleu1_score, bleu2_score, rouge_score, meteor_score = eval_epoch(model, crit, dataloader_test)\n",
    "\n",
    "print(f\"BLEU-1: {bleu1_score:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2_score:.4f}\")      \n",
    "print(f\"ROUGE-L: {rouge_score:.4f}\")\n",
    "print(f\"METEOR: {meteor_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ca4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
